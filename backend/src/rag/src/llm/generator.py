import subprocess


def generate(prompt: str, model: str = "llama3") -> str:
    """
    Sends a prompt to the Ollama LLM and returns the generated response.
    """

    process = subprocess.Popen(
        ["ollama", "run", model],
        stdin=subprocess.PIPE,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
        encoding="utf-8"
    )

    stdout, stderr = process.communicate(prompt)

    # Ollama often writes logs to stderr on Windows â€“ do NOT treat as failure
    if stdout and stdout.strip():
        return stdout.strip()

    # If truly nothing is generated, show stderr for debugging
    if stderr and stderr.strip():
        return f"Ollama error:\n{stderr.strip()}"

    return "No response generated by the model."
