import subprocess
import sys


def generate(prompt: str, model: str = "llama3") -> str:
    """
    Sends a prompt to the Ollama LLM and returns the generated response.
    """

    import os
    
    # Determine executable path
    ollama_cmd = "ollama"
    
    # Check if we are on Windows and if the default command might be missing from PATH
    if sys.platform == 'win32':
        # Try to find it in common install location if 'where' fails (which we handle via the try-except logic usually, but let's be proactive)
        local_app_data = os.environ.get('LOCALAPPDATA', '')
        potential_path = os.path.join(local_app_data, 'Programs', 'Ollama', 'ollama.exe')
        
        # If the file exists there, use the full path to be safe/robust
        if os.path.exists(potential_path):
             # We prefer the absolute path if we found it, to bypass PATH issues
             ollama_cmd = potential_path

    try:
        process = subprocess.Popen(
            [ollama_cmd, "run", model],
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            encoding="utf-8"
        )
        stdout, stderr = process.communicate(prompt)

        # Ollama often writes logs to stderr on Windows â€“ do NOT treat as failure
        if stdout and stdout.strip():
            return stdout.strip()

        # If truly nothing is generated, show stderr for debugging
        if stderr and stderr.strip():
            return f"Ollama error:\n{stderr.strip()}"

        return "No response generated by the model."

    except FileNotFoundError:
        return "Note: Local LLM (Ollama) is not installed or found in PATH. \n\nHowever, I can still show you the retrieved information (see Sources below). \n\nTo enable full AI answers, please install Ollama."
    except Exception as e:
        return f"Error generating response: {str(e)}"
